<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Object Detection for Autonomous Vehicles </title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0;
                padding: 0;
                line-height: 1.6;
                background-color: #f8f9fa;
                color: #333;
        
        
            }
        
            header {
                background: rgba(0, 17, 40, 0.86);
                box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(5.1px);
                -webkit-backdrop-filter: blur(5.1px);
                color: #fff;
                padding: 10px 0;
                text-align: center;
            }
        
            header h1 {
                margin: 0;
                font-size: 2.5em;
            }
        
            header p {
                font-size: 1.2em;
            }
        
            section {
                padding: 20px;
                margin: 20px 10px;
        
                background: rgba(255, 255, 255, 0.35);
                border-radius: 16px;
                box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
                backdrop-filter: blur(5.6px);
                -webkit-backdrop-filter: blur(5.6px);
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                border-radius: 8px;
            }
        
            section h2 {
                color: #007bff;
                border-bottom: 2px solid #007bff;
                padding-bottom: 10px;
            }
        
            section p,
            ul,
            ol {
                margin-bottom: 15px;
            }
        
            iframe {
                display: block;
                margin: 20px auto;
                width: 100%;
                height: 500px;
                border: none;
                border-radius: 8px;
            }
        
            footer {
                text-align: center;
                padding: 10px;
                background-color: #007bff;
                color: #fff;
            }
        
            footer a {
                color: #fff;
                text-decoration: underline;
            }
        
            .highlight {
                background-color: #f0f8ff;
                border-left: 5px solid #007bff;
                padding: 10px 15px;
                margin: 15px 0;
                font-style: italic;
            }
        
            .quote {
                font-size: 1.2em;
                font-weight: bold;
                color: #555;
                margin: 20px 0;
                text-align: center;
            }
        </style>
</head>
<header>
    <h1>Object Detection for Autonomous Vehicles</h1>
    <p>Building an advanced machine learning solution to enable reliable and efficient object detection for autonomous
        driving systems.</p>
</header>

<section>
    <h2>Introduction</h2>
    <p>
        This project focuses on developing a robust object detection system tailored for autonomous vehicles. The goal
        is to leverage machine learning techniques to accurately detect and classify objects in real-time, enabling
        safer and more efficient autonomous driving experiences.
    </p>
    <ul>
        <li>Primary Objective: Develop a scalable and efficient object detection pipeline for real-world autonomous
            vehicle systems.</li>
        <li>Dataset: Utilizes open-source and custom-curated datasets featuring road environments, vehicles,
            pedestrians, and traffic elements.</li>
        <li>Key Technologies: Python, TensorFlow/Keras, OpenCV, YOLO (You Only Look Once), and advanced preprocessing
            techniques.</li>
    </ul>
</section>

<section>
    <h2>KITTI Dataset</h2>
    <p>
        The KITTI dataset is one of the most widely used benchmarks for computer vision tasks in autonomous driving.
        Developed by the Karlsruhe Institute of Technology and Toyota Technological Institute, it provides extensive
        annotated data collected from real-world driving scenarios.
    </p>
    <ul>
        <li><strong>Contents:</strong> Includes stereo and monocular images, point cloud data from LiDAR sensors, and
            precise GPS information.</li>
        <li><strong>Annotation:</strong> Features bounding boxes, semantic segmentation masks, and 3D object annotations
            for cars, pedestrians, cyclists, and more.</li>
        <li><strong>Size:</strong> Over 15,000 labeled objects across 7481 training images and 7518 test images, with
            corresponding 3D LiDAR point clouds.</li>
        <li><strong>Challenges:</strong> Variations in lighting, occlusions, and perspective distortion make the dataset
            ideal for testing the robustness of object detection models.</li>
    </ul>
    <p>
        The KITTI dataset serves as a cornerstone for benchmarking and evaluating the performance of object detection
        algorithms in complex urban driving environments.
    </p>
    <img src="https://www.cvlibs.net/datasets/kitti/images/passat_sensors.jpg" alt="KITTI Dataset Sensors" />
</section>


<section>
    <h2>Preprocessing</h2>
    <p>
        Preprocessing is a critical step in preparing raw data for training and evaluation. This project implements
        advanced preprocessing techniques to ensure the model is fed with clean, consistent, and optimized inputs.
    </p>
    <ul>
        <li><strong>Image Resizing:</strong> Input images are resized to a fixed dimension, ensuring uniformity across
            the dataset.</li>
        <li><strong>Normalization:</strong> Pixel values are scaled to a range of 0 to 1 to improve model convergence
            during training.</li>
        <li><strong>Augmentation:</strong> Techniques such as rotation, flipping, and brightness adjustments are applied
            to enhance robustness against real-world variations.</li>
        <li><strong>Label Encoding:</strong> Ground truth annotations are converted into a format suitable for object
            detection models, including bounding box normalization.</li>
    </ul>
    <p>
        These preprocessing steps are designed to handle the complexities of autonomous driving scenarios, ensuring the
        model performs effectively under diverse conditions.
    </p>
</section>

<section>
    <h2>Data Conversion: COCO and YOLO Formats</h2>
    <p>
        Proper data formatting is crucial for training object detection models. This project involves converting
        annotations into two widely used formats: COCO and YOLO. Each format has specific requirements that facilitate
        compatibility with their respective frameworks.
    </p>

    <h3>Conversion to COCO Format</h3>
    <p>
        The COCO (Common Objects in Context) format is a versatile and detailed annotation format. It supports multiple
        object categories, segmentation masks, and keypoints for tasks like object detection and image segmentation. The
        conversion process involves:
    </p>
    <ul>
        <li>Standardizing annotations into the COCO JSON structure.</li>
        <li>Mapping object categories and bounding box coordinates.</li>
        <li>Ensuring compatibility with COCO evaluation metrics.</li>
    </ul>
    <img src="https://user-images.githubusercontent.com/9106252/37988554-1a3015fe-31f9-11e8-8eb6-21b0d462900c.png"
        alt="COCO Annotation Format Example" />

    <h3>Conversion to YOLO Format</h3>
    <p>
        The YOLO (You Only Look Once) format is optimized for real-time object detection. Its simplicity lies in using
        plain text files where each line represents an object annotation. Conversion steps include:
    </p>
    <ul>
        <li>Normalizing bounding box coordinates relative to image dimensions.</li>
        <li>Assigning object class IDs based on a predefined label map.</li>
        <li>Creating individual annotation files for each image.</li>
    </ul>
    <img src="https://miro.medium.com/v2/resize:fit:1114/1*aDCMw6f3MvyyoDhHejohwQ.png"
        alt="YOLO Annotation Format Example" />

    <p>
        By supporting both COCO and YOLO formats, the project ensures compatibility with a variety of object detection
        frameworks and simplifies model training workflows.
    </p>
</section>
<section>
    <h2>Validation</h2>
    <p>
        Validation is a critical step in evaluating the performance of the object detection model. It ensures that the
        model generalizes well to unseen data and identifies potential issues such as overfitting or underfitting. This
        project employs robust validation techniques to assess accuracy and reliability.
    </p>
    <ul>
        <li><strong>Dataset Split:</strong> The dataset is divided into training (80%) and validation (20%) subsets to
            evaluate performance on unseen data.</li>
        <li><strong>Metrics:</strong> Common evaluation metrics include mAP (mean Average Precision), IoU (Intersection
            over Union), and precision-recall curves.</li>
        <li><strong>Visualization:</strong> Predicted bounding boxes and confidence scores are compared against ground
            truth annotations to visually inspect model accuracy.</li>
        <li><strong>Early Stopping:</strong> Monitors validation loss to prevent overfitting by halting training when
            performance stagnates or deteriorates.</li>
    </ul>
    <p>
        Below is an example of a validation image showcasing predicted bounding boxes and their corresponding labels:
    </p>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953641/16/eyky0olv4vi2llbykbfg.jpg"
        alt="Validation Results with Predicted Bounding Boxes" />
</section>

<section>
    <h2>Training the Model Using YOLOv8</h2>
    <p>
        YOLOv8 (You Only Look Once, Version 8) is one of the most advanced object detection architectures, offering
        enhanced accuracy, speed, and efficiency. Training the model with YOLOv8 involves a structured workflow to
        optimize the detection capabilities for autonomous vehicles.
    </p>

    <h3>1. Model Architecture</h3>
    <p>
        YOLOv8 introduces significant improvements over its predecessors, with features like adaptive anchor boxes,
        deeper neural layers, and efficient CSP (Cross Stage Partial) connections. Its architecture ensures a balance
        between real-time inference speed and high detection accuracy, making it ideal for autonomous driving
        applications.
    </p>
    <ul>
        <li><strong>Backbone:</strong> CSPDarknet with enhanced feature extraction capabilities.</li>
        <li><strong>Neck:</strong> PANet (Path Aggregation Network) to merge feature maps at multiple scales.</li>
        <li><strong>Head:</strong> Optimized for precise bounding box regression and class predictions.</li>
    </ul>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953609/16/ww2c5qozlp40nmqixq60.jpg"
        alt="YOLOv8 Architecture Overview" />

    <h3>2. Training Workflow</h3>
    <p>
        The training process involves multiple steps to ensure the model learns effectively from the dataset:
    </p>
    <ul>
        <li><strong>Data Preparation:</strong> The dataset is converted into YOLO format, ensuring annotations are
            normalized and compatible with the framework.</li>
        <li><strong>Hyperparameter Optimization:</strong> Learning rate, batch size, and epoch count are tuned for
            optimal performance.</li>
        <li><strong>Augmentation:</strong> Real-time data augmentation is applied, including random cropping, flipping,
            and brightness adjustments, to improve model robustness.</li>
        <li><strong>Loss Functions:</strong> YOLOv8 uses a combination of localization loss (IoU-based), confidence
            loss, and classification loss to guide training.</li>
        <li><strong>Training Pipeline:</strong> The model is trained on GPUs, leveraging frameworks like PyTorch for
            efficient computation. Early stopping and learning rate schedulers are employed to fine-tune performance.
        </li>
    </ul>

    <h3>3. Evaluation During Training</h3>
    <p>
        During training, the model's performance is evaluated using metrics such as mAP (mean Average Precision) and
        loss values. Below is a correlation histogram showcasing the relationship between prediction confidence and
        ground truth during training:
    </p>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953608/16/vnvdaapzwxpgqmf07dxt.jpg"
        alt="Correlation Histogram of Predictions During Training" />

    <h3>4. Challenges and Solutions</h3>
    <p>
        Training object detection models on complex datasets like KITTI presents several challenges:
    </p>
    <ul>
        <li><strong>Class Imbalance:</strong> Certain classes (e.g., pedestrians or cyclists) may be underrepresented.
            This is mitigated using class-specific weighting during loss calculation.</li>
        <li><strong>Overfitting:</strong> Addressed using techniques like dropout layers, data augmentation, and early
            stopping.</li>
        <li><strong>Computational Load:</strong> Leveraging distributed training and mixed-precision computation reduces
            GPU memory usage and training time.</li>
    </ul>

    <h3>5. Results and Insights</h3>
    <p>
        The trained YOLOv8 model demonstrates exceptional performance, achieving high accuracy and real-time inference
        speeds. By effectively balancing speed and precision, the model is well-suited for integration into autonomous
        vehicle systems.
    </p>
</section>
    
    <section>
    <h2>Testing and Evaluation</h2>
    <p>
        The testing and evaluation phase is critical to assessing the real-world applicability of the object detection model. This phase involves rigorous testing across various scenarios to validate the model's robustness, accuracy, and reliability under diverse conditions. 
    </p>
    <h3>1. Real-World Performance</h3>
    <p>
        Testing in real-world scenarios is essential for understanding how the model performs under dynamic and unpredictable environments, such as changing lighting, weather conditions, and occlusions. The following video demonstrates the model's ability to accurately predict and classify objects in a real-time setting using footage from a car's dash cam:
    </p>
    <video controls>
        <source src="https://www.dropbox.com/scl/fi/ii2czikao2tc45sn9j51f/My-Movie.mp4?rlkey=li4qj9nypmovo95q96l9ihfva&st=08hmyb54&raw=1" type="video/mp4">
        Your browser does not support the video tag.
    </video>

    <h3>2. Quantitative Evaluation</h3>
    <p>
        The model's performance is evaluated using industry-standard metrics, such as mAP (mean Average Precision) and IoU (Intersection over Union). These metrics provide a comprehensive view of the model's detection capabilities. The graph below showcases the results obtained during testing, highlighting the precision and recall rates achieved:
    </p>
    <img src="https://www.dropbox.com/scl/fi/ai7h5zosce0c8b41fo5zh/results.png?rlkey=x5wu443ahd27aep6qhdipju9x&st=r4meorw1&raw=1" alt="Testing Results Graph" />

    <h3>3. Key Insights</h3>
    <p>
        Testing and evaluation provide valuable insights into the model's strengths and limitations. While the model demonstrates exceptional accuracy in detecting common road elements, challenges such as class imbalance and edge cases (e.g., partially visible objects) highlight areas for future improvement.
    </p>
    <p>
        These results confirm that the model is well-suited for deployment in autonomous vehicle systems, with robust performance in both controlled and real-world settings.
    </p>
</section>
    <section>
    <h2>Depth Detection</h2>
    <p>
        Depth detection is a critical component in understanding the spatial structure of an environment, which is essential for autonomous vehicles. By estimating the distance of objects from the vehicle, this functionality enhances decision-making and obstacle avoidance capabilities.
    </p>

    <h3>1. Overview</h3>
    <p>
        Depth detection in this project leverages the MiDaS (Monocular Depth Sensing) model, a state-of-the-art solution for estimating depth from single images. This approach provides accurate depth maps, even in challenging scenarios like varying lighting conditions and occlusions.
    </p>
    <ul>
        <li><strong>Model:</strong> MiDaS, optimized for monocular depth estimation.</li>
        <li><strong>Integration:</strong> Combined with YOLOv8 for simultaneous object detection and depth mapping.</li>
        <li><strong>Output:</strong> Normalized depth maps overlaid on input images for visualization.</li>
    </ul>

    <h3>2. Workflow</h3>
    <p>
        The depth detection process integrates seamlessly with the object detection pipeline. Key steps include:
    </p>
    <ul>
        <li>Loading and preprocessing input images, ensuring compatibility with MiDaS.</li>
        <li>Estimating depth values and generating normalized depth maps.</li>
        <li>Overlaying depth maps on original images for enhanced context.</li>
    </ul>

    <h3>3. Real-Time Visualization</h3>
    <p>
        During real-time inference, the system displays the following:
    </p>
    <ul>
        <li>Original frame with detected objects and bounding boxes.</li>
        <li>Depth map visualization using color gradients for better interpretation.</li>
    </ul>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1736332645/16/fvild7see7vygz7hwy0q.png" alt="Depth Detection Example" />

    <h3>4. Applications</h3>
    <p>
        Depth detection enhances the functionality of autonomous driving systems in several ways:
    </p>
    <ul>
        <li><strong>Collision Avoidance:</strong> By identifying the proximity of objects, the system can preemptively take action to avoid collisions.</li>
        <li><strong>Path Planning:</strong> Depth data helps in determining the optimal route by understanding the 3D layout of the environment.</li>
        <li><strong>Improved Object Detection:</strong> By combining depth and 2D data, the system can better differentiate between objects at varying distances.</li>
    </ul>

    <h3>5. Challenges and Future Directions</h3>
    <p>
        Depth detection introduces unique challenges, such as computational complexity and handling dynamic environments. Future work aims to:
    </p>
    <ul>
        <li>Improve the efficiency of depth estimation models for real-time applications.</li>
        <li>Incorporate stereo camera setups for enhanced depth accuracy.</li>
        <li>Expand datasets to include diverse scenarios for better generalization.</li>
    </ul>
</section>

    <section>
    <h2>Conclusion</h2>
    <p>
        Autonomous object detection systems represent a significant leap forward in the journey toward safer, more efficient, and intelligent transportation. By leveraging advanced machine learning models like YOLOv8 for object detection and MiDaS for depth estimation, these systems can accurately perceive and interpret their surroundings in real-time, enabling seamless navigation in complex and dynamic environments.
    </p>
    <h3>Key Benefits</h3>
    <ul>
        <li><strong>Enhanced Safety:</strong> By reliably detecting pedestrians, vehicles, and obstacles, these systems minimize the risk of collisions, even in challenging conditions such as poor visibility or high traffic density.</li>
        <li><strong>Efficient Navigation:</strong> Real-time depth perception and object classification enable vehicles to make informed decisions about speed, lane changes, and obstacle avoidance, optimizing travel efficiency.</li>
        <li><strong>Scalability:</strong> The modular design of these solutions ensures they can be adapted for various vehicle types, from personal cars to large-scale delivery fleets, facilitating widespread adoption.</li>
    </ul>
    <h3>Future Directions</h3>
    <p>
        As technology continues to evolve, autonomous object detection systems will benefit from:
    </p>
    <ul>
        <li><strong>Improved Algorithms:</strong> The integration of newer and more efficient models will enhance detection accuracy and reduce computational requirements.</li>
        <li><strong>Expanded Datasets:</strong> Training on more diverse datasets will improve system robustness across different geographies, weather conditions, and road types.</li>
        <li><strong>Collaboration with IoT:</strong> Leveraging connected vehicle infrastructure, such as smart traffic systems, can further augment detection capabilities and decision-making processes.</li>
    </ul>
    <p>
        The deployment of these technologies holds the promise of revolutionizing transportation, not only by improving road safety but also by transforming how goods and people move in our increasingly urbanized world. Continued research and development, coupled with robust testing and real-world validation, will pave the way for fully autonomous vehicles that are not only efficient but also universally trusted.
    </p>
</section>






<body>
    <footer>
        <p>Developed by Aryan Singh. Explore the full implementation on <a
                href="https://github.com/aryansingh920/object-detection-for-autonomous-vehicle"
                target="_blank">GitHub</a>.</p>
    </footer>
</body>

</html>
