<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Object Detection for Autonomous Vehicles </title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            background-color: #f8f9fa;
            margin: 0;
            color: #333;
        }

        header {
            background: rgba(0, 17, 40, 0.86);
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(5.1px);
            -webkit-backdrop-filter: blur(5.1px);
            color: #fff;
            text-align: center;
            padding: 20px;
        }

        header h1 {
            margin: 0;
            font-size: 2.5em;
        }

        section {
            background: rgba(255, 255, 255, 0.35);
            border-radius: 16px;
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            backdrop-filter: blur(5.6px);
            -webkit-backdrop-filter: blur(5.6px);
            margin: 20px auto;
            padding: 20px;
            max-width: 900px;
        }

        h2 {
            color: #007bff;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }

        img,
        video {
            display: block;
            margin: 20px auto;
            max-width: 100%;
            border-radius: 8px;
        }

        ul {
            margin: 10px 0;
            padding-left: 20px;
        }

        code {
            background-color: #f8f8f8;
            padding: 2px 6px;
            border: 1px solid #ddd;
            border-radius: 4px;
            font-family: Consolas, monospace;
        }

        footer {
            text-align: center;
            background-color: #007bff;
            color: #fff;
            padding: 10px;
        }

        footer a {
            color: #fff;
            text-decoration: underline;
        }
    </style>
</head>
<header>
    <h1>Object Detection for Autonomous Vehicles</h1>
    <p>Building an advanced machine learning solution to enable reliable and efficient object detection for autonomous
        driving systems.</p>
</header>

<section>
    <h2>Introduction</h2>
    <p>
        This project focuses on developing a robust object detection system tailored for autonomous vehicles. The goal
        is to leverage machine learning techniques to accurately detect and classify objects in real-time, enabling
        safer and more efficient autonomous driving experiences.
    </p>
    <ul>
        <li>Primary Objective: Develop a scalable and efficient object detection pipeline for real-world autonomous
            vehicle systems.</li>
        <li>Dataset: Utilizes open-source and custom-curated datasets featuring road environments, vehicles,
            pedestrians, and traffic elements.</li>
        <li>Key Technologies: Python, TensorFlow/Keras, OpenCV, YOLO (You Only Look Once), and advanced preprocessing
            techniques.</li>
    </ul>
</section>

<section>
    <h2>KITTI Dataset</h2>
    <p>
        The KITTI dataset is one of the most widely used benchmarks for computer vision tasks in autonomous driving.
        Developed by the Karlsruhe Institute of Technology and Toyota Technological Institute, it provides extensive
        annotated data collected from real-world driving scenarios.
    </p>
    <ul>
        <li><strong>Contents:</strong> Includes stereo and monocular images, point cloud data from LiDAR sensors, and
            precise GPS information.</li>
        <li><strong>Annotation:</strong> Features bounding boxes, semantic segmentation masks, and 3D object annotations
            for cars, pedestrians, cyclists, and more.</li>
        <li><strong>Size:</strong> Over 15,000 labeled objects across 7481 training images and 7518 test images, with
            corresponding 3D LiDAR point clouds.</li>
        <li><strong>Challenges:</strong> Variations in lighting, occlusions, and perspective distortion make the dataset
            ideal for testing the robustness of object detection models.</li>
    </ul>
    <p>
        The KITTI dataset serves as a cornerstone for benchmarking and evaluating the performance of object detection
        algorithms in complex urban driving environments.
    </p>
    <img src="https://www.cvlibs.net/datasets/kitti/images/passat_sensors.jpg" alt="KITTI Dataset Sensors" />
</section>


<section>
    <h2>Preprocessing</h2>
    <p>
        Preprocessing is a critical step in preparing raw data for training and evaluation. This project implements
        advanced preprocessing techniques to ensure the model is fed with clean, consistent, and optimized inputs.
    </p>
    <ul>
        <li><strong>Image Resizing:</strong> Input images are resized to a fixed dimension, ensuring uniformity across
            the dataset.</li>
        <li><strong>Normalization:</strong> Pixel values are scaled to a range of 0 to 1 to improve model convergence
            during training.</li>
        <li><strong>Augmentation:</strong> Techniques such as rotation, flipping, and brightness adjustments are applied
            to enhance robustness against real-world variations.</li>
        <li><strong>Label Encoding:</strong> Ground truth annotations are converted into a format suitable for object
            detection models, including bounding box normalization.</li>
    </ul>
    <p>
        These preprocessing steps are designed to handle the complexities of autonomous driving scenarios, ensuring the
        model performs effectively under diverse conditions.
    </p>
</section>

<section>
    <h2>Data Conversion: COCO and YOLO Formats</h2>
    <p>
        Proper data formatting is crucial for training object detection models. This project involves converting
        annotations into two widely used formats: COCO and YOLO. Each format has specific requirements that facilitate
        compatibility with their respective frameworks.
    </p>

    <h3>Conversion to COCO Format</h3>
    <p>
        The COCO (Common Objects in Context) format is a versatile and detailed annotation format. It supports multiple
        object categories, segmentation masks, and keypoints for tasks like object detection and image segmentation. The
        conversion process involves:
    </p>
    <ul>
        <li>Standardizing annotations into the COCO JSON structure.</li>
        <li>Mapping object categories and bounding box coordinates.</li>
        <li>Ensuring compatibility with COCO evaluation metrics.</li>
    </ul>
    <img src="https://user-images.githubusercontent.com/9106252/37988554-1a3015fe-31f9-11e8-8eb6-21b0d462900c.png"
        alt="COCO Annotation Format Example" />

    <h3>Conversion to YOLO Format</h3>
    <p>
        The YOLO (You Only Look Once) format is optimized for real-time object detection. Its simplicity lies in using
        plain text files where each line represents an object annotation. Conversion steps include:
    </p>
    <ul>
        <li>Normalizing bounding box coordinates relative to image dimensions.</li>
        <li>Assigning object class IDs based on a predefined label map.</li>
        <li>Creating individual annotation files for each image.</li>
    </ul>
    <img src="https://miro.medium.com/v2/resize:fit:1114/1*aDCMw6f3MvyyoDhHejohwQ.png"
        alt="YOLO Annotation Format Example" />

    <p>
        By supporting both COCO and YOLO formats, the project ensures compatibility with a variety of object detection
        frameworks and simplifies model training workflows.
    </p>
</section>
<section>
    <h2>Validation</h2>
    <p>
        Validation is a critical step in evaluating the performance of the object detection model. It ensures that the
        model generalizes well to unseen data and identifies potential issues such as overfitting or underfitting. This
        project employs robust validation techniques to assess accuracy and reliability.
    </p>
    <ul>
        <li><strong>Dataset Split:</strong> The dataset is divided into training (80%) and validation (20%) subsets to
            evaluate performance on unseen data.</li>
        <li><strong>Metrics:</strong> Common evaluation metrics include mAP (mean Average Precision), IoU (Intersection
            over Union), and precision-recall curves.</li>
        <li><strong>Visualization:</strong> Predicted bounding boxes and confidence scores are compared against ground
            truth annotations to visually inspect model accuracy.</li>
        <li><strong>Early Stopping:</strong> Monitors validation loss to prevent overfitting by halting training when
            performance stagnates or deteriorates.</li>
    </ul>
    <p>
        Below is an example of a validation image showcasing predicted bounding boxes and their corresponding labels:
    </p>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953641/16/eyky0olv4vi2llbykbfg.jpg"
        alt="Validation Results with Predicted Bounding Boxes" />
</section>

<section>
    <h2>Training the Model Using YOLOv8</h2>
    <p>
        YOLOv8 (You Only Look Once, Version 8) is one of the most advanced object detection architectures, offering
        enhanced accuracy, speed, and efficiency. Training the model with YOLOv8 involves a structured workflow to
        optimize the detection capabilities for autonomous vehicles.
    </p>

    <h3>1. Model Architecture</h3>
    <p>
        YOLOv8 introduces significant improvements over its predecessors, with features like adaptive anchor boxes,
        deeper neural layers, and efficient CSP (Cross Stage Partial) connections. Its architecture ensures a balance
        between real-time inference speed and high detection accuracy, making it ideal for autonomous driving
        applications.
    </p>
    <ul>
        <li><strong>Backbone:</strong> CSPDarknet with enhanced feature extraction capabilities.</li>
        <li><strong>Neck:</strong> PANet (Path Aggregation Network) to merge feature maps at multiple scales.</li>
        <li><strong>Head:</strong> Optimized for precise bounding box regression and class predictions.</li>
    </ul>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953609/16/ww2c5qozlp40nmqixq60.jpg"
        alt="YOLOv8 Architecture Overview" />

    <h3>2. Training Workflow</h3>
    <p>
        The training process involves multiple steps to ensure the model learns effectively from the dataset:
    </p>
    <ul>
        <li><strong>Data Preparation:</strong> The dataset is converted into YOLO format, ensuring annotations are
            normalized and compatible with the framework.</li>
        <li><strong>Hyperparameter Optimization:</strong> Learning rate, batch size, and epoch count are tuned for
            optimal performance.</li>
        <li><strong>Augmentation:</strong> Real-time data augmentation is applied, including random cropping, flipping,
            and brightness adjustments, to improve model robustness.</li>
        <li><strong>Loss Functions:</strong> YOLOv8 uses a combination of localization loss (IoU-based), confidence
            loss, and classification loss to guide training.</li>
        <li><strong>Training Pipeline:</strong> The model is trained on GPUs, leveraging frameworks like PyTorch for
            efficient computation. Early stopping and learning rate schedulers are employed to fine-tune performance.
        </li>
    </ul>

    <h3>3. Evaluation During Training</h3>
    <p>
        During training, the model's performance is evaluated using metrics such as mAP (mean Average Precision) and
        loss values. Below is a correlation histogram showcasing the relationship between prediction confidence and
        ground truth during training:
    </p>
    <img src="https://res.cloudinary.com/ddodpfjxe/image/upload/v1735953608/16/vnvdaapzwxpgqmf07dxt.jpg"
        alt="Correlation Histogram of Predictions During Training" />

    <h3>4. Challenges and Solutions</h3>
    <p>
        Training object detection models on complex datasets like KITTI presents several challenges:
    </p>
    <ul>
        <li><strong>Class Imbalance:</strong> Certain classes (e.g., pedestrians or cyclists) may be underrepresented.
            This is mitigated using class-specific weighting during loss calculation.</li>
        <li><strong>Overfitting:</strong> Addressed using techniques like dropout layers, data augmentation, and early
            stopping.</li>
        <li><strong>Computational Load:</strong> Leveraging distributed training and mixed-precision computation reduces
            GPU memory usage and training time.</li>
    </ul>

    <h3>5. Results and Insights</h3>
    <p>
        The trained YOLOv8 model demonstrates exceptional performance, achieving high accuracy and real-time inference
        speeds. By effectively balancing speed and precision, the model is well-suited for integration into autonomous
        vehicle systems.
    </p>
</section>
    
    <section>
    <h2>Testing and Evaluation</h2>
    <p>
        The testing and evaluation phase is critical to assessing the real-world applicability of the object detection model. This phase involves rigorous testing across various scenarios to validate the model's robustness, accuracy, and reliability under diverse conditions. 
    </p>
    <h3>1. Real-World Performance</h3>
    <p>
        Testing in real-world scenarios is essential for understanding how the model performs under dynamic and unpredictable environments, such as changing lighting, weather conditions, and occlusions. The following video demonstrates the model's ability to accurately predict and classify objects in a real-time setting using footage from a car's dash cam:
    </p>
    <video controls>
        <source src="https://www.dropbox.com/scl/fi/ii2czikao2tc45sn9j51f/My-Movie.mp4?rlkey=li4qj9nypmovo95q96l9ihfva&st=08hmyb54&raw=1" type="video/mp4">
        Your browser does not support the video tag.
    </video>

    <h3>2. Quantitative Evaluation</h3>
    <p>
        The model's performance is evaluated using industry-standard metrics, such as mAP (mean Average Precision) and IoU (Intersection over Union). These metrics provide a comprehensive view of the model's detection capabilities. The graph below showcases the results obtained during testing, highlighting the precision and recall rates achieved:
    </p>
    <img src="https://www.dropbox.com/scl/fi/ai7h5zosce0c8b41fo5zh/results.png?rlkey=x5wu443ahd27aep6qhdipju9x&st=r4meorw1&raw=1" alt="Testing Results Graph" />

    <h3>3. Key Insights</h3>
    <p>
        Testing and evaluation provide valuable insights into the model's strengths and limitations. While the model demonstrates exceptional accuracy in detecting common road elements, challenges such as class imbalance and edge cases (e.g., partially visible objects) highlight areas for future improvement.
    </p>
    <p>
        These results confirm that the model is well-suited for deployment in autonomous vehicle systems, with robust performance in both controlled and real-world settings.
    </p>
</section>




<body>
    <footer>
        <p>Developed by Aryan Singh. Explore the full implementation on <a
                href="https://github.com/aryansingh920/object-detection-for-autonomous-vehicle"
                target="_blank">GitHub</a>.</p>
    </footer>
</body>

</html>
